# -*- coding: utf-8 -*-
"""main.py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kvs2dzuqMpMAB5Tte16dx-noYd4OZAdN
"""

pip install torch

# transformer_from_scratch/main.py

import torch
import torch.nn as nn
import torch.nn.functional as F
import math

# Positional Encoding
def get_positional_encoding(seq_len, d_model):
    pos = torch.arange(seq_len, dtype=torch.float).unsqueeze(1)
    i = torch.arange(d_model, dtype=torch.float).unsqueeze(0)
    angle_rates = 1 / torch.pow(10000, (2 * (i // 2)) / d_model)
    angle_rads = pos * angle_rates
    angle_rads[:, 0::2] = torch.sin(angle_rads[:, 0::2])
    angle_rads[:, 1::2] = torch.cos(angle_rads[:, 1::2])
    return angle_rads.unsqueeze(0)

# Scaled Dot-Product Attention
def scaled_dot_product_attention(q, k, v, mask=None):
    d_k = q.size(-1)
    scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k)
    if mask is not None:
        scores = scores.masked_fill(mask == 0, float('-inf'))
    attn = F.softmax(scores, dim=-1)
    return torch.matmul(attn, v), attn

# Multi-head Attention
class MultiHeadAttention(nn.Module):
    def __init__(self, d_model, num_heads):
        super().__init__()
        assert d_model % num_heads == 0
        self.d_k = d_model // num_heads
        self.num_heads = num_heads
        self.q_linear = nn.Linear(d_model, d_model)
        self.k_linear = nn.Linear(d_model, d_model)
        self.v_linear = nn.Linear(d_model, d_model)
        self.out = nn.Linear(d_model, d_model)

    def forward(self, q, k, v, mask=None):
        B, L, D = q.size()
        q = self.q_linear(q).view(B, L, self.num_heads, self.d_k).transpose(1, 2)
        k = self.k_linear(k).view(B, L, self.num_heads, self.d_k).transpose(1, 2)
        v = self.v_linear(v).view(B, L, self.num_heads, self.d_k).transpose(1, 2)
        scores, attn = scaled_dot_product_attention(q, k, v, mask)
        concat = scores.transpose(1, 2).contiguous().view(B, L, D)
        return self.out(concat)

# Feed Forward Network
class FeedForward(nn.Module):
    def __init__(self, d_model, d_ff=2048):
        super().__init__()
        self.linear1 = nn.Linear(d_model, d_ff)
        self.linear2 = nn.Linear(d_ff, d_model)

    def forward(self, x):
        return self.linear2(F.relu(self.linear1(x)))

# Transformer Encoder Layer
class EncoderLayer(nn.Module):
    def __init__(self, d_model, num_heads, d_ff):
        super().__init__()
        self.attn = MultiHeadAttention(d_model, num_heads)
        self.ff = FeedForward(d_model, d_ff)
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)

    def forward(self, x, mask=None):
        x2 = self.attn(x, x, x, mask)
        x = self.norm1(x + x2)
        x2 = self.ff(x)
        x = self.norm2(x + x2)
        return x

# Full Transformer Encoder
class TransformerEncoder(nn.Module):
    def __init__(self, vocab_size, d_model, num_layers, num_heads, d_ff, max_len=512):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, d_model)
        self.pos_encoding = get_positional_encoding(max_len, d_model)
        self.layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff) for _ in range(num_layers)])
        self.norm = nn.LayerNorm(d_model)
        self.linear_out = nn.Linear(d_model, vocab_size)

    def forward(self, x, mask=None):
        x = self.embedding(x) + self.pos_encoding[:, :x.size(1), :].to(x.device)
        for layer in self.layers:
            x = layer(x, mask)
        return self.linear_out(self.norm(x))

# Training loop on a toy copy task
def train_copy_task():
    vocab_size = 50
    seq_len = 10
    d_model = 64
    num_layers = 2
    num_heads = 4
    d_ff = 128

    model = TransformerEncoder(vocab_size, d_model, num_layers, num_heads, d_ff)
    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)
    criterion = nn.CrossEntropyLoss()
    model.train()

    for epoch in range(100):
        x = torch.randint(1, vocab_size, (32, seq_len))
        y = x.clone()
        logits = model(x)
        loss = criterion(logits.view(-1, vocab_size), y.view(-1))

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        if (epoch + 1) % 10 == 0:
            print(f"Epoch {epoch + 1}: Loss = {loss.item():.4f}")

# Entry point
if __name__ == '__main__':
    train_copy_task()